{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameter tunning using Optuna and CNN implementation\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatest(Dataset):\n",
    "  def __init__(self,features,labels):\n",
    "\n",
    "    self.features=torch.tensor(features,dtype=torch.float32,device=device)\n",
    "    self.labels=torch.tensor(features,dtype=torch.long,device=device)\n",
    "\n",
    "  def __len__(self,features):\n",
    "    return len(self.features)\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.features[index],self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=CustomDataset(X_train,y_train)\n",
    "test_data=CustomDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_load=DataLoader(dataset=dataset,shuffle=True,batch_size=32,pin_memory=True)\n",
    "test_data_load=DataLoader(dataset=test_data,shuffle=False,batch_size=32,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FlexibileMyNN(nn.Moemory=True)dule):\n",
    "\n",
    "  def __init__(self,input_dims,output_dims,hidden_number_layers,neuron_per_layer,dropout_rate):\n",
    "\n",
    "    #calling the parent class constructor\n",
    "    super().__init__()\n",
    "\n",
    "    #making  the model\n",
    "\n",
    "    #collecting all the layers in a list\n",
    "    layers=[]\n",
    "\n",
    "    #looping throught the hidden layers\n",
    "    #in each iteration we add the hidden layer to the layers list\n",
    "    for i in range(hidden_number_layers):\n",
    "\n",
    "      layers.append(nn.Linear(input_dims,neuron_per_layer))\n",
    "      layers.append(nn.BatchNorm1d(neuron_per_layer))\n",
    "      layers.append(nn.ReLU())\n",
    "      layers.append(nn.Dropout(dropout_rate))\n",
    "      #making sure tat in the next loop the outputlayer becomes the input\n",
    "      input_dims=neuron_per_layer\n",
    "\n",
    "    layers.append(nn.Linear(neuron_per_layer,output_dims))\n",
    "\n",
    "    self.model=nn.Sequential(*layers)\n",
    "\n",
    "    #forward props\n",
    "  def forward(self,features):\n",
    "\n",
    "      output=self.model(features)\n",
    "\n",
    "      return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective_function(trial):\n",
    "\n",
    "  #next hyperparams suggestions\n",
    "  neuron_per_layer=trial.suggest_int('neurons',8,264,step=8)\n",
    "  hidden_number_layers=trial.suggest_int('hidden_layers',1,10)\n",
    "  epochs=trial.suggest_int('epochs',20,120,step=20)\n",
    "  learning_rate=trial.suggest_int('lr',0.1,0.5)\n",
    "  weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2,log=True)\n",
    "  dropout_rate=trial.suggest_int('droput',2,6,step=1)\n",
    "\n",
    "  #input and output_dims\n",
    "\n",
    "  input_dims=784\n",
    "  output_dims=10\n",
    "\n",
    "  #model init\n",
    "\n",
    "  model=FlexibileMyNN(input_dims,output_dims=output_dims,hidden_number_layers=hidden_number_layers,neuron_per_layer=neuron_per_layer,dropout_rate=dropout_rate)\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  #params\n",
    "\n",
    "  learning_rate=learning_rate\n",
    "\n",
    "  epochs=epochs\n",
    "\n",
    "  loss=nn.CrossEntropyLoss()\n",
    "\n",
    "  #optimizer selection\n",
    "\n",
    "  optimizer=torch.optim.SGD(params=model.parameters(),weight_decay=weight_decay,lr=learning_rate)\n",
    "\n",
    "  #dataset_loading\n",
    "\n",
    "  training_data_load=DataLoader(dataset=dataset,shuffle=True,batch_size=32,pin_memory=True)\n",
    "  test_data_load=DataLoader(dataset=test,shuffle=False,batch_size=32,pin_memory=True)\n",
    "\n",
    "  #training loop\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    for batch_features,batch_labels in training_data_load:\n",
    "\n",
    "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
    "\n",
    "      #forward_propagation\n",
    "      prediction=model(batch_features)\n",
    "\n",
    "      #lloss_calculation\n",
    "      loss_output=loss(prediction,batch_labels)\n",
    "\n",
    "      #zero_optimizers\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      #backward_prop\n",
    "      loss_output.backward()\n",
    "\n",
    "      #Optimize\n",
    "      optimizer.step()\n",
    "\n",
    "  #evaluation\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  total=0\n",
    "  correct=0\n",
    "\n",
    "  #evaluation loop on test data\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for  batch_features,batch_labels in test_data_load:\n",
    "\n",
    "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
    "\n",
    "      #forward_propagation\n",
    "      prediction=model(batch_features)\n",
    "\n",
    "      #finding the maximum probability\n",
    "      _,prediction=torch.max(prediction,1)\n",
    "\n",
    "      #total size\n",
    "      total+=batch_features.shape[0]\n",
    "\n",
    "      #correct predictions\n",
    "      correct+=(prediction==batch_labels).sum().item()\n",
    "\n",
    "  accuracy=correct/total\n",
    "\n",
    "  #return the accuracy to optimize\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948212ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna as opt\n",
    "\n",
    "pruner=opt.pruners.MedianPruner(n_warmup_steps=5)\n",
    "\n",
    "study=opt.create_study(direction='maximize',pruner=pruner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective_function,n_trials=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
