{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a45e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58d16ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ed0a8d0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "54dffe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available:\n",
    "    device=torch.device('mps')\n",
    "else:\n",
    "    device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e5210e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bacb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path('/Users/divyyadav/Desktop/pytorch/src/ann,cnn,rnn,lstm/100_Unique_QA_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bb6e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16871d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bfed4306-5829-4413-aa4d-fdd65937d5dc",
       "rows": [
        [
         "0",
         "What is the capital of France?",
         "Paris"
        ],
        [
         "1",
         "What is the capital of Germany?",
         "Berlin"
        ],
        [
         "2",
         "Who wrote 'To Kill a Mockingbird'?",
         "Harper-Lee"
        ],
        [
         "3",
         "What is the largest planet in our solar system?",
         "Jupiter"
        ],
        [
         "4",
         "What is the boiling point of water in Celsius?",
         "100"
        ],
        [
         "5",
         "Who painted the Mona Lisa?",
         "Leonardo-da-Vinci"
        ],
        [
         "6",
         "What is the square root of 64?",
         "8"
        ],
        [
         "7",
         "What is the chemical symbol for gold?",
         "Au"
        ],
        [
         "8",
         "Which year did World War II end?",
         "1945"
        ],
        [
         "9",
         "What is the longest river in the world?",
         "Nile"
        ],
        [
         "10",
         "What is the capital of Japan?",
         "Tokyo"
        ],
        [
         "11",
         "Who developed the theory of relativity?",
         "Albert-Einstein"
        ],
        [
         "12",
         "What is the freezing point of water in Fahrenheit?",
         "32"
        ],
        [
         "13",
         "Which planet is known as the Red Planet?",
         "Mars"
        ],
        [
         "14",
         "Who is the author of '1984'?",
         "George-Orwell"
        ],
        [
         "15",
         "What is the currency of the United Kingdom?",
         "Pound"
        ],
        [
         "16",
         "What is the capital of India?",
         "Delhi"
        ],
        [
         "17",
         "Who discovered gravity?",
         "Newton"
        ],
        [
         "18",
         "How many continents are there on Earth?",
         "7"
        ],
        [
         "19",
         "Which gas do plants use for photosynthesis?",
         "CO2"
        ],
        [
         "20",
         "What is the smallest prime number?",
         "2"
        ],
        [
         "21",
         "Who invented the telephone?",
         "Alexander-Graham-Bell"
        ],
        [
         "22",
         "What is the capital of Australia?",
         "Canberra"
        ],
        [
         "23",
         "Which ocean is the largest?",
         "Pacific-Ocean"
        ],
        [
         "24",
         "What is the speed of light in vacuum?",
         "299,792,458m/s"
        ],
        [
         "25",
         "Which language is spoken in Brazil?",
         "Portuguese"
        ],
        [
         "26",
         "Who discovered penicillin?",
         "Alexander-Fleming"
        ],
        [
         "27",
         "What is the capital of Canada?",
         "Ottawa"
        ],
        [
         "28",
         "What is the largest mammal on Earth?",
         "Whale"
        ],
        [
         "29",
         "Which element has the atomic number 1?",
         "Hydrogen"
        ],
        [
         "30",
         "What is the tallest mountain in the world?",
         "Everest"
        ],
        [
         "31",
         "Which city is known as the Big Apple?",
         "NewYork"
        ],
        [
         "32",
         "How many planets are in the Solar System?",
         "8"
        ],
        [
         "33",
         "Who painted 'Starry Night'?",
         "vangogh"
        ],
        [
         "34",
         "What is the chemical formula of water?",
         "H2O"
        ],
        [
         "35",
         "What is the capital of Italy?",
         "Rome"
        ],
        [
         "36",
         "Which country is famous for sushi?",
         "Japan"
        ],
        [
         "37",
         "Who was the first person to step on the Moon?",
         "Armstrong"
        ],
        [
         "38",
         "What is the main ingredient in guacamole?",
         "Avocado"
        ],
        [
         "39",
         "How many sides does a hexagon have?",
         "6"
        ],
        [
         "40",
         "What is the currency of China?",
         "Yuan"
        ],
        [
         "41",
         "Who wrote 'Pride and Prejudice'?",
         "Jane-Austen"
        ],
        [
         "42",
         "What is the chemical symbol for iron?",
         "Fe"
        ],
        [
         "43",
         "What is the hardest natural substance on Earth?",
         "Diamond"
        ],
        [
         "44",
         "Which continent is the largest by area?",
         "Asia"
        ],
        [
         "45",
         "Who was the first President of the United States?",
         "George-Washington"
        ],
        [
         "46",
         "Which bird is known for its ability to mimic sounds?",
         "Parrot"
        ],
        [
         "47",
         "What is the longest-running animated TV show?",
         "Simpsons"
        ],
        [
         "48",
         "What is the smallest country in the world?",
         "VaticanCity"
        ],
        [
         "49",
         "Which planet has the most moons?",
         "Saturn"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 90
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Who directed the movie 'Titanic'?</td>\n",
       "      <td>JamesCameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Which superhero is also known as the Dark Knight?</td>\n",
       "      <td>Batman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>What is the capital of Brazil?</td>\n",
       "      <td>Brasilia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Which fruit is known as the king of fruits?</td>\n",
       "      <td>Mango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Which country is known for the Eiffel Tower?</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question        answer\n",
       "0                      What is the capital of France?         Paris\n",
       "1                     What is the capital of Germany?        Berlin\n",
       "2                  Who wrote 'To Kill a Mockingbird'?    Harper-Lee\n",
       "3     What is the largest planet in our solar system?       Jupiter\n",
       "4      What is the boiling point of water in Celsius?           100\n",
       "..                                                ...           ...\n",
       "85                  Who directed the movie 'Titanic'?  JamesCameron\n",
       "86  Which superhero is also known as the Dark Knight?        Batman\n",
       "87                     What is the capital of Brazil?      Brasilia\n",
       "88        Which fruit is known as the king of fruits?         Mango\n",
       "89       Which country is known for the Eiffel Tower?        France\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64dc0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method DataFrame.info of                                              question        answer\n",
       " 0                      What is the capital of France?         Paris\n",
       " 1                     What is the capital of Germany?        Berlin\n",
       " 2                  Who wrote 'To Kill a Mockingbird'?    Harper-Lee\n",
       " 3     What is the largest planet in our solar system?       Jupiter\n",
       " 4      What is the boiling point of water in Celsius?           100\n",
       " ..                                                ...           ...\n",
       " 85                  Who directed the movie 'Titanic'?  JamesCameron\n",
       " 86  Which superhero is also known as the Dark Knight?        Batman\n",
       " 87                     What is the capital of Brazil?      Brasilia\n",
       " 88        Which fruit is known as the king of fruits?         Mango\n",
       " 89       Which country is known for the Eiffel Tower?        France\n",
       " \n",
       " [90 rows x 2 columns]>,\n",
       " <bound method NDFrame.describe of                                              question        answer\n",
       " 0                      What is the capital of France?         Paris\n",
       " 1                     What is the capital of Germany?        Berlin\n",
       " 2                  Who wrote 'To Kill a Mockingbird'?    Harper-Lee\n",
       " 3     What is the largest planet in our solar system?       Jupiter\n",
       " 4      What is the boiling point of water in Celsius?           100\n",
       " ..                                                ...           ...\n",
       " 85                  Who directed the movie 'Titanic'?  JamesCameron\n",
       " 86  Which superhero is also known as the Dark Knight?        Batman\n",
       " 87                     What is the capital of Brazil?      Brasilia\n",
       " 88        Which fruit is known as the king of fruits?         Mango\n",
       " 89       Which country is known for the Eiffel Tower?        France\n",
       " \n",
       " [90 rows x 2 columns]>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info,df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing tokenization\n",
    "def tokenization(text):\n",
    "    lower_text=text.lower()\n",
    "    text=lower_text.replace('','')\n",
    "    text=lower_text.replace('?','')\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b7f7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={'<UNK>':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cb994aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making vocabulary\n",
    "def  build_vocab(row):\n",
    "\n",
    "    tokenized_row=tokenization(row['question'])\n",
    "    tokenized_question=tokenization(row['answer'])\n",
    "\n",
    "    merged_tokens=tokenized_row+tokenized_question\n",
    "    \n",
    "    for token in merged_tokens:\n",
    "\n",
    "        if token not in vocab:\n",
    "            \n",
    "            vocab[token]=len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "771a9729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "eb8721ea-0e14-486a-bce2-97591eac590e",
       "rows": [
        [
         "0",
         null
        ],
        [
         "1",
         null
        ],
        [
         "2",
         null
        ],
        [
         "3",
         null
        ],
        [
         "4",
         null
        ],
        [
         "5",
         null
        ],
        [
         "6",
         null
        ],
        [
         "7",
         null
        ],
        [
         "8",
         null
        ],
        [
         "9",
         null
        ],
        [
         "10",
         null
        ],
        [
         "11",
         null
        ],
        [
         "12",
         null
        ],
        [
         "13",
         null
        ],
        [
         "14",
         null
        ],
        [
         "15",
         null
        ],
        [
         "16",
         null
        ],
        [
         "17",
         null
        ],
        [
         "18",
         null
        ],
        [
         "19",
         null
        ],
        [
         "20",
         null
        ],
        [
         "21",
         null
        ],
        [
         "22",
         null
        ],
        [
         "23",
         null
        ],
        [
         "24",
         null
        ],
        [
         "25",
         null
        ],
        [
         "26",
         null
        ],
        [
         "27",
         null
        ],
        [
         "28",
         null
        ],
        [
         "29",
         null
        ],
        [
         "30",
         null
        ],
        [
         "31",
         null
        ],
        [
         "32",
         null
        ],
        [
         "33",
         null
        ],
        [
         "34",
         null
        ],
        [
         "35",
         null
        ],
        [
         "36",
         null
        ],
        [
         "37",
         null
        ],
        [
         "38",
         null
        ],
        [
         "39",
         null
        ],
        [
         "40",
         null
        ],
        [
         "41",
         null
        ],
        [
         "42",
         null
        ],
        [
         "43",
         null
        ],
        [
         "44",
         null
        ],
        [
         "45",
         null
        ],
        [
         "46",
         null
        ],
        [
         "47",
         null
        ],
        [
         "48",
         null
        ],
        [
         "49",
         null
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 90
       }
      },
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(build_vocab,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63e9335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " \"'to\": 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " \"mockingbird'\": 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " \"'1984'\": 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " \"'starry\": 130,\n",
       " \"night'\": 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'to': 143,\n",
       " 'step': 144,\n",
       " 'moon': 145,\n",
       " 'armstrong': 146,\n",
       " 'main': 147,\n",
       " 'ingredient': 148,\n",
       " 'guacamole': 149,\n",
       " 'avocado': 150,\n",
       " 'sides': 151,\n",
       " 'does': 152,\n",
       " 'hexagon': 153,\n",
       " 'have': 154,\n",
       " '6': 155,\n",
       " 'china': 156,\n",
       " 'yuan': 157,\n",
       " \"'pride\": 158,\n",
       " 'and': 159,\n",
       " \"prejudice'\": 160,\n",
       " 'jane-austen': 161,\n",
       " 'iron': 162,\n",
       " 'fe': 163,\n",
       " 'hardest': 164,\n",
       " 'natural': 165,\n",
       " 'substance': 166,\n",
       " 'diamond': 167,\n",
       " 'continent': 168,\n",
       " 'by': 169,\n",
       " 'area': 170,\n",
       " 'asia': 171,\n",
       " 'president': 172,\n",
       " 'states': 173,\n",
       " 'george-washington': 174,\n",
       " 'bird': 175,\n",
       " 'its': 176,\n",
       " 'ability': 177,\n",
       " 'mimic': 178,\n",
       " 'sounds': 179,\n",
       " 'parrot': 180,\n",
       " 'longest-running': 181,\n",
       " 'animated': 182,\n",
       " 'tv': 183,\n",
       " 'show': 184,\n",
       " 'simpsons': 185,\n",
       " 'vaticancity': 186,\n",
       " 'most': 187,\n",
       " 'moons': 188,\n",
       " 'saturn': 189,\n",
       " \"'romeo\": 190,\n",
       " \"juliet'\": 191,\n",
       " 'shakespeare': 192,\n",
       " \"earth's\": 193,\n",
       " 'atmosphere': 194,\n",
       " 'nitrogen': 195,\n",
       " 'bones': 196,\n",
       " 'adult': 197,\n",
       " 'human': 198,\n",
       " 'body': 199,\n",
       " '206': 200,\n",
       " 'metal': 201,\n",
       " 'liquid': 202,\n",
       " 'at': 203,\n",
       " 'room': 204,\n",
       " 'temperature': 205,\n",
       " 'mercury': 206,\n",
       " 'russia': 207,\n",
       " 'moscow': 208,\n",
       " 'electricity': 209,\n",
       " 'benjamin-franklin': 210,\n",
       " 'second-largest': 211,\n",
       " 'land': 212,\n",
       " 'color': 213,\n",
       " 'ripe': 214,\n",
       " 'banana': 215,\n",
       " 'yellow': 216,\n",
       " 'month': 217,\n",
       " '28': 218,\n",
       " 'days': 219,\n",
       " 'common': 220,\n",
       " 'february': 221,\n",
       " 'study': 222,\n",
       " 'living': 223,\n",
       " 'organisms': 224,\n",
       " 'called': 225,\n",
       " 'biology': 226,\n",
       " 'home': 227,\n",
       " 'great': 228,\n",
       " 'wall': 229,\n",
       " 'bees': 230,\n",
       " 'collect': 231,\n",
       " 'from': 232,\n",
       " 'flowers': 233,\n",
       " 'nectar': 234,\n",
       " 'opposite': 235,\n",
       " \"'day'\": 236,\n",
       " 'night': 237,\n",
       " 'south': 238,\n",
       " 'korea': 239,\n",
       " 'seoul': 240,\n",
       " 'bulb': 241,\n",
       " 'edison': 242,\n",
       " 'humans': 243,\n",
       " 'breathe': 244,\n",
       " 'survival': 245,\n",
       " 'oxygen': 246,\n",
       " '144': 247,\n",
       " '12': 248,\n",
       " 'pyramids': 249,\n",
       " 'giza': 250,\n",
       " 'egypt': 251,\n",
       " 'sea': 252,\n",
       " 'creature': 253,\n",
       " 'eight': 254,\n",
       " 'arms': 255,\n",
       " 'octopus': 256,\n",
       " 'holiday': 257,\n",
       " 'celebrated': 258,\n",
       " 'december': 259,\n",
       " '25': 260,\n",
       " 'christmas': 261,\n",
       " 'yen': 262,\n",
       " 'legs': 263,\n",
       " 'spider': 264,\n",
       " 'sport': 265,\n",
       " 'uses': 266,\n",
       " 'net,': 267,\n",
       " 'ball,': 268,\n",
       " 'hoop': 269,\n",
       " 'basketball': 270,\n",
       " 'kangaroos': 271,\n",
       " 'female': 272,\n",
       " 'minister': 273,\n",
       " 'uk': 274,\n",
       " 'margaretthatcher': 275,\n",
       " 'fastest': 276,\n",
       " 'animal': 277,\n",
       " 'cheetah': 278,\n",
       " 'periodic': 279,\n",
       " 'table': 280,\n",
       " 'spain': 281,\n",
       " 'madrid': 282,\n",
       " 'closest': 283,\n",
       " 'sun': 284,\n",
       " 'father': 285,\n",
       " 'computers': 286,\n",
       " 'charlesbabbage': 287,\n",
       " 'mexico': 288,\n",
       " 'mexicocity': 289,\n",
       " 'colors': 290,\n",
       " 'rainbow': 291,\n",
       " 'musical': 292,\n",
       " 'instrument': 293,\n",
       " 'black': 294,\n",
       " 'white': 295,\n",
       " 'keys': 296,\n",
       " 'piano': 297,\n",
       " 'americas': 298,\n",
       " '1492': 299,\n",
       " 'christophercolumbus': 300,\n",
       " 'disney': 301,\n",
       " 'character': 302,\n",
       " 'long': 303,\n",
       " 'nose': 304,\n",
       " 'grows': 305,\n",
       " 'it': 306,\n",
       " 'when': 307,\n",
       " 'lying': 308,\n",
       " 'pinocchio': 309,\n",
       " 'directed': 310,\n",
       " 'movie': 311,\n",
       " \"'titanic'\": 312,\n",
       " 'jamescameron': 313,\n",
       " 'superhero': 314,\n",
       " 'also': 315,\n",
       " 'dark': 316,\n",
       " 'knight': 317,\n",
       " 'batman': 318,\n",
       " 'brasilia': 319,\n",
       " 'fruit': 320,\n",
       " 'king': 321,\n",
       " 'fruits': 322,\n",
       " 'mango': 323,\n",
       " 'eiffel': 324,\n",
       " 'tower': 325}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c19ffe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a7b7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a functions that returns the indice value from the text after iterating the tokens \n",
    "def index(text,vocab):\n",
    "    \n",
    "    indexed_text=[]\n",
    "\n",
    "    for tokens in tokenization(text):\n",
    "\n",
    "        if tokens in vocab:\n",
    "            indexed_text.append(vocab[tokens])\n",
    "\n",
    "        else:\n",
    "            indexed_text.append(vocab['<UNK>'])\n",
    "\n",
    "    return indexed_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1eb9d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to convert the question and answer from the data to indexs\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d894467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatase(Dataset):\n",
    "\n",
    "    def __init__(self,df,vocab):\n",
    "        super().__init__()\n",
    "        self.text=df\n",
    "        self.vocab=vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]\n",
    "    \n",
    "    def __getitem__(self,indx):\n",
    "\n",
    "        question=self.text.iloc[indx]['question']\n",
    "      \n",
    "        answer=self.text.iloc[indx]['answer']\n",
    "\n",
    "        question_list=index(question,self.vocab)\n",
    "\n",
    "        answer_list=index(answer,self.vocab)\n",
    "        \n",
    "        #conver to tensor \n",
    "        return torch.tensor(question_list),torch.tensor(answer_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f014810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=CustomDatase(df,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do padding over the questions and answers embedding \n",
    "def collate_fn(batch):\n",
    "            \"\"\"\n",
    "            Custom collate function to pad variable length sequences.\n",
    "            \n",
    "            Args:\n",
    "                batch: A list of tuples, where each tuple is (question_tensor, answer_tensor)\n",
    "            \"\"\"\n",
    "           \n",
    "            # Step A: Separate the data and the labels\n",
    "                # We unzip the list of tuples into two separate lists.\n",
    "                # batch_data will be [tensor([1,2]), tensor([3,4,5,6]), ...]\n",
    "                # batch_labels will be [0, 1, 0]\n",
    " \n",
    "            question,answer=zip(*batch)\n",
    "           \n",
    "            # Step B: Pad the data\n",
    "            # Since the tensors are different lengths, we cannot stack them yet.\n",
    "            # pad_sequence will add zeros to the shorter ones so they match the longest one.\n",
    "            # batch_first=True means the output shape is (Batch_Size, Max_Length)\n",
    "            question_paded=pad_sequence(question,batch_first=True)\n",
    "             \n",
    "            if answer[0].dim()>0:\n",
    "                answer_padded=pad_sequence(answer,batch_first=True)\n",
    "\n",
    "            else:\n",
    "               # Step C: Stack the labels\n",
    "                # Labels are usually just single numbers (integers), so we can simply \n",
    "                # turn the list into a Tensor.\n",
    "               answer_padded=torch.stack(answer)\n",
    "\n",
    "            return question_paded,answer_padded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2438129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader\n",
    "training_data_loader=DataLoader(dataset=data,batch_size=32,shuffle=True,pin_memory=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f2d6ef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   2,   3,   4,   5, 113,   0,   0,   0],\n",
      "        [  1,   2,   3,  33,  34,   5,  35,   0,   0],\n",
      "        [  1,   2,   3, 141, 117,  83,   3, 279, 280],\n",
      "        [  1,   2,   3,  69,   5,   3,  70,  71,   0],\n",
      "        [ 42, 137,   2, 138,  39, 176, 271,   0,   0],\n",
      "        [  1,   2,   3,  69,   5, 156,   0,   0,   0],\n",
      "        [  1,  87, 230, 231, 232, 233,   0,   0,   0],\n",
      "        [ 42, 265, 266,  14, 267, 268, 159, 269,   0],\n",
      "        [  1,   2,   3, 235,   5, 236,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5, 281,   0,   0,   0],\n",
      "        [ 10,  75, 111,   0,   0,   0,   0,   0,   0],\n",
      "        [ 10,  29,   3,  30,  31,   0,   0,   0,   0],\n",
      "        [ 78,  79, 151, 152,  14, 153, 154,   0,   0],\n",
      "        [ 10,  11,  12,  13,  14,  15,   0,   0,   0],\n",
      "        [ 42, 137,   2, 138,  39, 139,   0,   0,   0],\n",
      "        [  1,   2,   3,  37,  38,  39, 162,   0,   0],\n",
      "        [  1,   2,   3,   4,   5, 288,   0,   0,   0],\n",
      "        [  1,   2,   3, 222,   5, 223, 224, 225,   0],\n",
      "        [ 10,   2,  62,  63,   3, 285,   5, 286,   0],\n",
      "        [ 42, 137,   2,  62,  39,   3, 324, 325,   0],\n",
      "        [  1,   2,   3,   4,   5, 135,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5, 109,   0,   0,   0],\n",
      "        [ 42, 292, 293, 118, 294, 159, 295, 296,   0],\n",
      "        [  1,   2,   3,  17, 115,  83,  84,   0,   0],\n",
      "        [ 10,  75,   3, 298,  19, 299,   0,   0,   0],\n",
      "        [  1,   2,   3,  37, 133,   5,  26,   0,   0],\n",
      "        [ 42, 320,   2,  62,  63,   3, 321,   5, 322],\n",
      "        [ 42,  86,  87,  88,  89,  39,  90,   0,   0],\n",
      "        [ 10,  29, 130, 131,   0,   0,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5,   8,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5, 238, 239,   0,   0],\n",
      "        [  1,   2,   3,  59,  25,   5,  26,  19,  60]]) tensor([[114],\n",
      "        [ 36],\n",
      "        [121],\n",
      "        [ 72],\n",
      "        [ 99],\n",
      "        [157],\n",
      "        [234],\n",
      "        [270],\n",
      "        [237],\n",
      "        [282],\n",
      "        [112],\n",
      "        [ 32],\n",
      "        [155],\n",
      "        [ 16],\n",
      "        [ 53],\n",
      "        [163],\n",
      "        [289],\n",
      "        [226],\n",
      "        [287],\n",
      "        [  6],\n",
      "        [136],\n",
      "        [319],\n",
      "        [297],\n",
      "        [116],\n",
      "        [300],\n",
      "        [134],\n",
      "        [323],\n",
      "        [ 91],\n",
      "        [132],\n",
      "        [  9],\n",
      "        [240],\n",
      "        [ 61]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22,   0,   0],\n",
      "        [ 42,  18,   2,   3, 283, 143,   3, 284,   0,   0],\n",
      "        [ 78,  79, 196,  81,  19,   3, 197, 198, 199,   0],\n",
      "        [ 78,  79, 263, 152,  14, 264, 154,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5, 207,   0,   0,   0,   0],\n",
      "        [ 42,   2,   3, 211, 137, 169, 212, 170,   0,   0],\n",
      "        [  1,   2,   3, 164, 165, 166,  83,  84,   0,   0],\n",
      "        [ 10, 140,   3, 141, 142, 143, 144,  83,   3, 145],\n",
      "        [ 42, 314,   2, 315,  62,  63,   3, 316, 317,   0],\n",
      "        [ 42, 107,   2, 108,  19, 109,   0,   0,   0,   0],\n",
      "        [  1,   2,   3,  33,  34,   5, 247,   0,   0,   0],\n",
      "        [  1,   2,   3,  92,  93,  94,   0,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5,  73,   0,   0,   0,   0],\n",
      "        [  1,   2,   3, 147,  86,  19, 193, 194,   0,   0],\n",
      "        [  1,   2,   3,  50,  51,  19,   3,  45,   0,   0],\n",
      "        [  1,   2,   3,  69,   5,  53,   0,   0,   0,   0],\n",
      "        [ 42, 137, 118,   3, 249,   5, 250,   0,   0,   0],\n",
      "        [ 42, 201,   2,  14, 202, 203, 204, 205,   0,   0],\n",
      "        [  1,   2,   3,   4,   5,   6,   0,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5,  99,   0,   0,   0,   0],\n",
      "        [ 10, 310,   3, 311, 312,   0,   0,   0,   0,   0],\n",
      "        [ 10,  11, 158, 159, 160,   0,   0,   0,   0,   0],\n",
      "        [ 10,  75, 209,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 10,  96,   3,  97,   0,   0,   0,   0,   0,   0],\n",
      "        [ 42, 101,   2,   3,  17,   0,   0,   0,   0,   0],\n",
      "        [ 42, 137,   2, 227, 143,   3, 228, 229,   0,   0],\n",
      "        [  1,   2,   3, 103,   5, 104,  19, 105,   0,   0],\n",
      "        [ 42,  43,  44,  45,  46,  47,  48,   0,   0,   0],\n",
      "        [ 78,  79, 290,  81,  19,  14, 291,   0,   0,   0],\n",
      "        [ 10,  75,  76,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,   2,   3,  37,  38,  39,  40,   0,   0,   0],\n",
      "        [  1,   2,   3, 147, 148,  19, 149,   0,   0,   0]]) tensor([[ 36],\n",
      "        [206],\n",
      "        [200],\n",
      "        [ 36],\n",
      "        [208],\n",
      "        [113],\n",
      "        [167],\n",
      "        [146],\n",
      "        [318],\n",
      "        [110],\n",
      "        [248],\n",
      "        [ 95],\n",
      "        [ 74],\n",
      "        [195],\n",
      "        [ 52],\n",
      "        [262],\n",
      "        [251],\n",
      "        [206],\n",
      "        [  7],\n",
      "        [100],\n",
      "        [313],\n",
      "        [161],\n",
      "        [210],\n",
      "        [ 98],\n",
      "        [102],\n",
      "        [156],\n",
      "        [106],\n",
      "        [ 49],\n",
      "        [ 85],\n",
      "        [ 77],\n",
      "        [ 41],\n",
      "        [150]])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45,   0,   0,   0,   0],\n",
      "        [ 10, 140,   3, 141, 272,  93, 273,   5,   3, 274,   0,   0],\n",
      "        [  1,   2,   3, 181, 182, 183, 184,   0,   0,   0,   0,   0],\n",
      "        [ 42, 168,   2,   3,  17, 169, 170,   0,   0,   0,   0,   0],\n",
      "        [ 10,  55,   3,  56,   5,  57,   0,   0,   0,   0,   0,   0],\n",
      "        [ 42, 175,   2,  62,  39, 176, 177, 143, 178, 179,   0,   0],\n",
      "        [ 42, 217, 118, 218, 219,  19,  14, 220,  43,   0,   0,   0],\n",
      "        [  1,   2,   3, 213,   5,  14, 214, 215,   0,   0,   0,   0],\n",
      "        [ 10,   2,   3,  66,   5,  67,   0,   0,   0,   0,   0,   0],\n",
      "        [ 42, 125,   2,  62,  63,   3, 126, 127,   0,   0,   0,   0],\n",
      "        [ 78,  79,  80,  81,  82,  83,  84,   0,   0,   0,   0,   0],\n",
      "        [ 42, 257,   2, 258,  83, 259, 260,   0,   0,   0,   0,   0],\n",
      "        [ 10,  11, 190, 159, 191,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,   2,   3,  17,  18,  19,  20,  21,  22,   0,   0,   0],\n",
      "        [ 42,  86,  87, 243, 244,  19,  39, 245,   0,   0,   0,   0],\n",
      "        [ 10, 140,   3, 141, 172,   5,   3,  70, 173,   0,   0,   0],\n",
      "        [ 42, 252, 253, 118, 254, 255,   0,   0,   0,   0,   0,   0],\n",
      "        [ 10,  96,   3, 104, 241,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 42, 117, 118,   3, 119,  94, 120,   0,   0,   0,   0,   0],\n",
      "        [ 42,  18, 118,   3, 187, 188,   0,   0,   0,   0,   0,   0],\n",
      "        [ 42, 301, 302, 118,  14, 303, 304, 159, 305, 306, 307, 308],\n",
      "        [ 42,  18,   2,  62,  63,   3,  64,  18,   0,   0,   0,   0],\n",
      "        [  1,   2,   3,  24,  25,   5,  26,  19,  27,   0,   0,   0],\n",
      "        [  1,   2,   3,   4,   5,  53,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,   2,   3, 122, 123,  19,   3,  45,   0,   0,   0,   0],\n",
      "        [ 42,   2,   3, 276, 212, 277,   0,   0,   0,   0,   0,   0]]) tensor([[186],\n",
      "        [275],\n",
      "        [185],\n",
      "        [171],\n",
      "        [ 58],\n",
      "        [180],\n",
      "        [221],\n",
      "        [216],\n",
      "        [ 68],\n",
      "        [128],\n",
      "        [ 85],\n",
      "        [261],\n",
      "        [192],\n",
      "        [ 23],\n",
      "        [246],\n",
      "        [174],\n",
      "        [256],\n",
      "        [242],\n",
      "        [121],\n",
      "        [189],\n",
      "        [309],\n",
      "        [ 65],\n",
      "        [ 28],\n",
      "        [ 54],\n",
      "        [124],\n",
      "        [278]])\n"
     ]
    }
   ],
   "source": [
    "for questio,answer in training_data_loader:\n",
    "    print(questio,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "dfe72752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings=nn.Embedding(vocab_size,52)\n",
    "\n",
    "        self.rnn=nn.RNN(52,30,batch_first=True)\n",
    "\n",
    "        self.output=nn.Linear(30,vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self,text):\n",
    "          \n",
    "        embeddings=self.embeddings(text)\n",
    "\n",
    "        hidden_output,final_output=self.rnn(embeddings)\n",
    "\n",
    "        output=self.output(final_output.squeeze(0))\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "58c1bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model\n",
    "t=nn.Embedding(324,52)\n",
    "z=nn.RNN(52,30)\n",
    "y=nn.Linear(30,324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "be260644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 6, 52])\n",
      "torch.Size([1, 6, 30])\n"
     ]
    }
   ],
   "source": [
    "input=data[0][0].reshape(1,6)\n",
    "\n",
    "print(input.shape)\n",
    "\n",
    "m=t(input)\n",
    "\n",
    "print(m.shape)\n",
    "\n",
    "x,c=z(m)\n",
    "print(c.shape)\n",
    "\n",
    "n=y(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7a29e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RNN(len(vocab)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "56de5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "optimizer=optim.Adam(model.parameters(),lr=lr)\n",
    "los=nn.CrossEntropyLoss()\n",
    "epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "17d232ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyyadav/Desktop/pytorch/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[260]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m features\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#loss calculation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m loss=\u001b[43mlos\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#backward propagation\u001b[39;00m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/pytorch/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "for epocs in range(epochs):\n",
    "\n",
    "    total_loss=0\n",
    "\n",
    "    for features,labels in training_data_loader:\n",
    "\n",
    "        features,labels=features.to(device),labels.to(device)\n",
    "\n",
    "        #forward propagaton\n",
    "        output=model(features)\n",
    "\n",
    "        #print(output.shape,labels.shape)\n",
    "        features\n",
    "\n",
    "        #loss calculation\n",
    "        loss=los(output,labels)\n",
    "\n",
    "        #backward propagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        loss+=loss.item()\n",
    "        \n",
    "    print(f'loss for {epocs+1} is {loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d6e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
