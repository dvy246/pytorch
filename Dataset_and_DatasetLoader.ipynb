{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvy246/pytorch/blob/main/Dataset_and_DatasetLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBXq4ns262OU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # For data manipulation and analysis, especially with DataFrames.\n",
        "from sklearn.model_selection import train_test_split # For splitting data into training and testing sets.\n",
        "import matplotlib.pyplot as plt # For creating static, animated, and interactive visualizations.\n",
        "import torch # The main PyTorch library for building neural networks.\n",
        "import torch.nn as nn # Neural network modules and layers (e.g., Linear, ReLU, BatchNorm1d).\n",
        "import torch.optim as optim # Optimization algorithms (e.g., SGD, Adam).\n",
        "from torch.utils.data import Dataset,DataLoader # Utilities for data loading and batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5bX8ReKV8Zp"
      },
      "outputs": [],
      "source": [
        "# Determine if a GPU (CUDA) is available for accelerated computing, otherwise use the CPU.\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU0Ci429WEhp",
        "outputId": "dda3b894-602e-4713-db62-7788cc9a9ce0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAefdmAYlgAV",
        "outputId": "de9fd107-ed4c-4201-fff7-9449a834b47d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79a8f45ff290>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Set a manual seed for PyTorch's random number generator.\n",
        "# This ensures reproducibility of results, meaning the same code run multiple times\n",
        "# will produce the same initial weights and data shuffles, leading to consistent outcomes.\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "pFpFWpzBhDlE",
        "outputId": "bac2dae1-29d8-42ab-a47a-3d64b87929d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/fashion-mnist_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2668301791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the training data from the CSV file into a pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# on_bad_lines='skip' is used to skip any lines that cause parsing errors due to inconsistent formatting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/fashion-mnist_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the shape of the DataFrame (number of rows, number of columns).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/fashion-mnist_train.csv'"
          ]
        }
      ],
      "source": [
        "# Read the training data from the CSV file into a pandas DataFrame.\n",
        "# on_bad_lines='skip' is used to skip any lines that cause parsing errors due to inconsistent formatting.\n",
        "df1=pd.read_csv('/content/fashion-mnist_train.csv', on_bad_lines='skip')\n",
        "# Display the shape of the DataFrame (number of rows, number of columns).\n",
        "df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8vwR94vaJp6"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rStD-lIgnk5x"
      },
      "outputs": [],
      "source": [
        "df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvdySo3KX8D2"
      },
      "outputs": [],
      "source": [
        "# Read the test data from the CSV file into a pandas DataFrame.\n",
        "# on_bad_lines='skip' is used to skip any lines that cause parsing errors due to inconsistent formatting.\n",
        "df2=pd.read_csv('/content/fashion-mnist_test.csv', on_bad_lines='skip')\n",
        "# Display the shape of the DataFrame (number of rows, number of columns).\n",
        "df2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2O-l-d0Zv-P"
      },
      "outputs": [],
      "source": [
        "# Concatenate the training and test DataFrames (df1 and df2) vertically.\n",
        "# This combines both datasets into a single DataFrame 'df' for initial processing.\n",
        "df=pd.concat([df1,df2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBTtNeJAZ0tI"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-sfK621dM1a"
      },
      "outputs": [],
      "source": [
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZp1Ei8mYBmH"
      },
      "outputs": [],
      "source": [
        "# Create an 8x8 grid of subplots for displaying images\n",
        "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "\n",
        "# Set a dynamic super title for the entire figure\n",
        "# It displays the number of images plotted, which is 64 (8*8)\n",
        "num_images = 8 * 8\n",
        "plt.suptitle(f'{num_images} Sample Images from the Dataset', size=20)\n",
        "\n",
        "# Loop through each subplot in the grid and display an image\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # Extract image data (pixels) from the DataFrame, skipping the label column\n",
        "    # Reshape the 1D pixel array into a 28x28 image for display\n",
        "    image = df.iloc[i, 1:].values.reshape(28, 28)\n",
        "\n",
        "    # Display the image in the current subplot\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Remove the axis ticks and labels for a cleaner visual\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Set the title of each subplot to show the corresponding image's label\n",
        "    ax.set_title(f'Label: {df.iloc[i, 0]}')\n",
        "\n",
        "# Adjust subplot parameters for a tight layout, ensuring elements do not overlap\n",
        "# The rect parameter leaves space for the suptitle\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjusted rect to accommodate suptitle\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytf8ncFalkJ9"
      },
      "outputs": [],
      "source": [
        "# Define features (X) and labels (y) from the combined DataFrame.\n",
        "# X will contain all pixel columns (from the second column onwards, index 1 to end).\n",
        "X=df.iloc[:,1:]\n",
        "# y will contain the 'label' column (the first column, index 0).\n",
        "y=df.iloc[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZnLw_A6lqN_"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAaAoqCNlsnn"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQI-TjQxiL3p"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets.\n",
        "# X_train, y_train: Features and labels for training (80% of the data).\n",
        "# X_test, y_test: Features and labels for testing (20% of the data).\n",
        "# test_size=0.2 specifies that 20% of the data will be used for the test set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiv8NSFLUwZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "1ad542de-cdf1-4c48-ab99-1b2544c6f42e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2534926350.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Pixel values typically range from 0 to 255. Dividing by 255 normalizes them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# to a range of 0 to 1. This helps in faster and more stable training of neural networks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Scale the pixel values of the training and testing features.\n",
        "# Pixel values typically range from 0 to 255. Dividing by 255 normalizes them\n",
        "# to a range of 0 to 1. This helps in faster and more stable training of neural networks.\n",
        "X_train=X_train/255.0\n",
        "X_test=X_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8OOiEy1Y6al"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class to handle the image features and labels.\n",
        "# This class inherits from torch.utils.data.Dataset and provides methods\n",
        "# for accessing individual data samples.\n",
        "class CustomDatest(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels):\n",
        "    # Constructor: Initializes the dataset with features and labels.\n",
        "    # Converts pandas DataFrames to PyTorch tensors with appropriate data types.\n",
        "    # features are converted to float32 (common for neural network inputs).\n",
        "    # labels are converted to long (required for CrossEntropyLoss).\n",
        "    self.features=torch.tensor(features.to_numpy(),dtype=torch.float32)\n",
        "    self.labels=torch.tensor(labels.to_numpy() ,dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    # Returns the total number of samples in the dataset.\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self,indx):\n",
        "    # Returns a single sample (feature-label pair) at the given index.\n",
        "    return self.features[indx],self.labels[indx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9HH_OJWayUt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "40d1f013-07b6-4a6b-a65f-a696a79c5866"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2118205985.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCustomDatest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCustomDatest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "dataset=CustomDatest(X_train,y_train)\n",
        "test=CustomDatest(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "GiKjsBU8a61s",
        "outputId": "0d10f10e-7d98-43f5-9337-4c605e3c154b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2411785438.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBjL3cIBbPtV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "ffd75674-9b86-4500-c5bc-38edecd50700"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-181097782.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# batch_size=32: Processes 32 samples at a time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pin_memory=True: Transfers data to GPU faster by 'pinning' it in CPU RAM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtraining_data_load\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# test_data_load: DataLoader for the test dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Create DataLoader objects for efficient batching and loading of data during training and testing.\n",
        "# DataLoader handles shuffling, batching, and loading data in parallel.\n",
        "\n",
        "# training_data_load: DataLoader for the training dataset.\n",
        "# dataset: The custom dataset object for training data.\n",
        "# shuffle=True: Shuffles the data at each epoch to improve generalization and prevent overfitting.\n",
        "# batch_size=32: Processes 32 samples at a time.\n",
        "# pin_memory=True: Transfers data to GPU faster by 'pinning' it in CPU RAM.\n",
        "training_data_load=DataLoader(dataset=dataset,shuffle=True,batch_size=32,pin_memory=True)\n",
        "\n",
        "# test_data_load: DataLoader for the test dataset.\n",
        "# test: The custom dataset object for test data.\n",
        "# shuffle=False: No need to shuffle test data, as order does not affect evaluation.\n",
        "# batch_size=32: Processes 32 samples at a time.\n",
        "# pin_memory=True: Transfers data to GPU faster by 'pinning' it in CPU RAM.\n",
        "test_data_load=DataLoader(dataset=test,shuffle=False,batch_size=32,pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oklfMgoJcY04"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.activation import ReLU\n",
        "\n",
        "# Define the neural network architecture.\n",
        "# MyNN inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n",
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self,num_features):\n",
        "    # Constructor: Initializes the layers of the neural network.\n",
        "    super().__init__() # Call the constructor of the parent class (nn.Module).\n",
        "\n",
        "    # Define the sequential model, which is a container for modules that will be\n",
        "    # added in the order they are passed in the constructor.\n",
        "    self.Model=nn.Sequential(\n",
        "\n",
        "        # First Linear layer: Input features (e.g., 784 pixels) to 128 neurons.\n",
        "        nn.Linear(num_features,128),\n",
        "        # ReLU activation function: Introduces non-linearity.\n",
        "        nn.ReLU(),\n",
        "        # BatchNorm1d: Normalizes outputs from the previous layer to stabilize and speed up training.\n",
        "        nn.BatchNorm1d(128),\n",
        "        # Second Linear layer: 128 neurons to 80 neurons.\n",
        "        nn.Linear(128,80),\n",
        "        # ReLU activation.\n",
        "        nn.ReLU(),\n",
        "        # BatchNorm1d.\n",
        "        nn.BatchNorm1d(80),\n",
        "        # Dropout layer: Randomly sets a fraction (p=0.4) of input units to 0 at each update\n",
        "        # during training. This helps prevent overfitting.\n",
        "        nn.Dropout(p=0.4),\n",
        "        # Third Linear layer: 80 neurons to 60 neurons.\n",
        "        nn.Linear(60,20),\n",
        "        # ReLU activation.\n",
        "        nn.ReLU(),\n",
        "        # Final Linear layer: 20 neurons to 10 neurons (output classes for Fashion MNIST).\n",
        "        nn.Linear(20,10)\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    def forward(self,features):\n",
        "      return self.Model(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HQO9Bi0eEhs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "ad286284-b606-45ea-ed69-f4c99e284893"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2816336692.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the neural network model with the number of input features (pixels).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMyNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Move the model to the specified device (GPU if available, else CPU) for computation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize the neural network model with the number of input features (pixels).\n",
        "model=MyNN(X_train.shape[1])\n",
        "\n",
        "# Move the model to the specified device (GPU if available, else CPU) for computation.\n",
        "model.to(device)\n",
        "\n",
        "# Set the learning rate for the optimizer. This controls the step size during weight updates.\n",
        "lr=0.1\n",
        "# Define the number of training epochs (full passes over the training dataset).\n",
        "epochs=1\n",
        "\n",
        "# Define the Loss Function: CrossEntropyLoss is commonly used for multi-class classification problems.\n",
        "# It combines LogSoftmax and NLLLoss (Negative Log Likelihood Loss).\n",
        "Loss=nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the Optimizer: Stochastic Gradient Descent (SGD) is used to update model parameters.\n",
        "# params=model.parameters(): Specifies which parameters the optimizer should update.\n",
        "# lr=lr: Sets the learning rate.\n",
        "# weight_decay=1e-4: Adds L2 regularization (a small penalty to the loss for large weights)\n",
        "# to help prevent overfitting.\n",
        "optimizer=optim.SGD(params=model.parameters(),lr=lr,weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "Jusj7zdHd7Wf",
        "outputId": "bc402c0f-9016-4462-eab0-643845614ac5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4135561103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepocs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Initialize total loss for the current epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtotal_epoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
          ]
        }
      ],
      "source": [
        "for epocs in range(epochs):\n",
        "\n",
        "  # Initialize total loss for the current epoch.\n",
        "  total_epoch_loss=0\n",
        "\n",
        "  # Iterate through batches of training data.\n",
        "  for batch_features,batch_labels in training_data_load:\n",
        "\n",
        "                  # Move batch features and labels to the specified device (GPU/CPU).\n",
        "                  batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "\n",
        "                  # Forward pass: Compute model output (predictions) for the current batch.\n",
        "                  output=model(batch_features)\n",
        "\n",
        "                  # Loss calculation: Compute the loss between model output and true labels.\n",
        "                  loss=Loss(output,batch_labels)\n",
        "\n",
        "                  # Zero gradients: Clear previous gradients before computing new ones.\n",
        "                  # This is crucial because PyTorch accumulates gradients by default.\n",
        "                  optimizer.zero_grad()\n",
        "\n",
        "                  # Backward pass: Compute gradients of the loss with respect to model parameters.\n",
        "                  loss.backward()\n",
        "\n",
        "                  # Optimizer step: Update model parameters using the computed gradients and learning rate.\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # Accumulate loss for the current epoch.\n",
        "                  total_epoch_loss+=loss.item()\n",
        "\n",
        "\n",
        "  # Calculate the average loss for the current epoch.\n",
        "  avg_loss=total_epoch_loss/len(training_data_load)\n",
        "\n",
        "  # Print the epoch number and the average training loss.\n",
        "  print(f'epoch {epocs+1} and the avg loss {avg_loss:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTNbNnOiS2fJ",
        "outputId": "3aac22e3-49c6-4fa5-f5b3-a9046d3f17ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnC1J3yIhaaX"
      },
      "outputs": [],
      "source": [
        "import torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "EdYaTd92jYV5",
        "outputId": "e6fc1d55-404e-4166-82ce-46dfe1d679a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3036031071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorchinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "torchinfo.summary(model=model,input_size=(1,X_train.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "model=Path('Model')\n",
        "\n",
        "model.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "model_name='new.pth'\n",
        "\n",
        "final=model/model_name\n",
        "\n"
      ],
      "metadata": {
        "id": "VdrgkP6-MBLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "id": "f16dB-BRMWBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIi3lwS4kcg3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "253a6155-e49f-4672-e7bd-644b1755aa08"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1910932387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(),final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez4C-F3BjmlR",
        "outputId": "d16030b0-6590-44e8-c856-68198e10402f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.87\n"
          ]
        }
      ],
      "source": [
        "# Set the model to evaluation mode. This disables dropout and batch normalization layers\n",
        "# if they are present in the model, as these behave differently during training and evaluation.\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the total number of samples and correctly predicted samples.\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# Disable gradient calculations for evaluation. This is important because we don't need\n",
        "# to compute gradients during inference, which saves memory and speeds up computations.\n",
        "with torch.no_grad():\n",
        "  # Iterate through the test data using the DataLoader, which provides batches of features and labels.\n",
        "  for batch_features, batch_lables in test_data_load:\n",
        "\n",
        "    #gpu acceleration - Move both features and labels to the device\n",
        "    batch_features, batch_lables = batch_features.to(device), batch_lables.to(device)\n",
        "\n",
        "    # Perform a forward pass to get raw model outputs (logits) for the current batch.\n",
        "    y_prediction = model(batch_features)\n",
        "\n",
        "    # Get the predicted class by finding the index of the maximum value along dimension 1 (the class dimension).\n",
        "    # torch.max returns both the maximum value and its index. We only need the index.\n",
        "    _, prediction = torch.max(y_prediction, 1)\n",
        "\n",
        "    # Update the total number of samples processed.\n",
        "    total += batch_lables.shape[0] # batch_lables.shape[0] gives the number of samples in the current batch.\n",
        "\n",
        "    # Count how many predictions in the current batch match the true labels.\n",
        "    # (prediction == batch_lables) creates a boolean tensor, .sum() counts True values,\n",
        "    # and .item() converts the single-element tensor to a Python number.\n",
        "    correct += (prediction == batch_lables).sum().item()\n",
        "\n",
        "# After iterating through all test data, calculate the overall accuracy.\n",
        "# Accuracy is the ratio of correctly predicted samples to the total number of samples.\n",
        "print(round(correct / total,2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIVxT4VHQu98",
        "outputId": "b699e94a-19c7-4696-c41e-8405ee53c45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.89\n"
          ]
        }
      ],
      "source": [
        "# Set the model to evaluation mode. This disables dropout and batch normalization layers\n",
        "# if they are present in the model, as these behave differently during training and evaluation.\n",
        "model.eval() # Ensure the model is in evaluation mode.\n",
        "\n",
        "# Initialize variables to keep track of the total number of samples and correctly predicted samples.\n",
        "# Resetting total and correct here for an accurate calculation of training accuracy after the test accuracy calculation.\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# Disable gradient calculations for evaluation. This is important because we don't need\n",
        "# to compute gradients during inference, which saves memory and speeds up computations.\n",
        "with torch.no_grad():\n",
        "  # Iterate through the training data using the DataLoader, which provides batches of features and labels.\n",
        "  for batch_features, batch_lables in training_data_load: # Using training_data_load here for training accuracy\n",
        "\n",
        "    # GPU acceleration - Move both features and labels to the device\n",
        "    batch_features, batch_lables = batch_features.to(device), batch_lables.to(device)\n",
        "\n",
        "    # Perform a forward pass to get raw model outputs (logits) for the current batch.\n",
        "    y_prediction = model(batch_features)\n",
        "\n",
        "    # Get the predicted class by finding the index of the maximum value along dimension 1 (the class dimension).\n",
        "    # torch.max returns both the maximum value and its index. We only need the index.\n",
        "    _, prediction = torch.max(y_prediction, 1)\n",
        "\n",
        "    # Update the total number of samples processed.\n",
        "    total += batch_lables.shape[0] # batch_lables.shape[0] gives the number of samples in the current batch.\n",
        "\n",
        "    # Count how many predictions in the current batch match the true labels.\n",
        "    # (prediction == batch_lables) creates a boolean tensor, .sum() counts True values,\n",
        "    # and .item() converts the single-element tensor to a Python number.\n",
        "    correct += (prediction == batch_lables).sum().item()\n",
        "\n",
        "# After iterating through all training data, calculate the overall training accuracy.\n",
        "# Accuracy is the ratio of correctly predicted samples to the total number of samples.\n",
        "print(round(correct / total,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHcZLhEOH8Vm"
      },
      "source": [
        "# HyperParameter tunning using Optuna and CNN implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYBxtKmwf8gC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrGdUXbaY32",
        "outputId": "4cc55252-de51-4b88-f581-2bd12835e2df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC_HO9tGY-J8"
      },
      "outputs": [],
      "source": [
        "class CustomDatest(Dataset):\n",
        "  def __init__(self,features,labels):\n",
        "\n",
        "    self.features=torch.tensor(features,dtype=torch.float32,device=device)\n",
        "    self.labels=torch.tensor(features,dtype=torch.long,device=device)\n",
        "\n",
        "  def __len__(self,features):\n",
        "    return len(self.features)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.features[index],self.labels[index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PDlOWFHiitc"
      },
      "outputs": [],
      "source": [
        "training_data_load=DataLoader(dataset=dataset,shuffle=True,batch_size=32,pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2c2iFdHTFYd"
      },
      "outputs": [],
      "source": [
        "class FlexibileMyNN(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dims,output_dims,hidden_number_layers,neuron_per_layer,dropout_rate):\n",
        "\n",
        "    #calling the parent class constructor\n",
        "    super().__init__()\n",
        "\n",
        "    #making  the model\n",
        "\n",
        "    #collecting all the layers in a list\n",
        "    layers=[]\n",
        "\n",
        "    #looping throught the hidden layers\n",
        "    #in each iteration we add the hidden layer to the layers list\n",
        "    for i in range(hidden_number_layers):\n",
        "\n",
        "      layers.append(nn.Linear(input_dims,neuron_per_layer))\n",
        "      layers.append(nn.BatchNorm1d(neuron_per_layer))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(dropout_rate))\n",
        "      #making sure tat in the next loop the outputlayer becomes the input\n",
        "      input_dims=neuron_per_layer\n",
        "\n",
        "    layers.append(nn.Linear(neuron_per_layer,output_dims))\n",
        "\n",
        "    self.model=nn.Sequential(*layers)\n",
        "\n",
        "    #forward props\n",
        "  def forward(self,features):\n",
        "\n",
        "      output=self.model(features)\n",
        "\n",
        "      return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPlyCHJ6IHap"
      },
      "outputs": [],
      "source": [
        "from os import truncate\n",
        "#objective function\n",
        "\n",
        "def objective_function(trial):\n",
        "\n",
        "  #next hyperparams suggestions\n",
        "  neuron_per_layer=trial.suggest_int('neurons',8,264,step=8)\n",
        "  hidden_number_layers=trial.suggest_int('hidden_layers',1,10)\n",
        "  epochs=trial.suggest_int('epochs',20,120,step=20)\n",
        "  learning_rate=trial.suggest_int('lr',0.1,0.5)\n",
        "  weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2,log=True)\n",
        "  dropout_rate=trial.suggest_int('droput',2,6,step=1)\n",
        "\n",
        "  #input and output_dims\n",
        "\n",
        "  input_dims=784\n",
        "  output_dims=10\n",
        "\n",
        "  #model init\n",
        "\n",
        "  model=FlexibileMyNN(input_dims,output_dims=output_dims,hidden_number_layers=hidden_number_layers,neuron_per_layer=neuron_per_layer,dropout_rate=dropout_rate)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  #params\n",
        "\n",
        "  learning_rate=learning_rate\n",
        "\n",
        "  epochs=epochs\n",
        "\n",
        "  loss=nn.CrossEntropyLoss()\n",
        "\n",
        "  #optimizer selection\n",
        "\n",
        "  optimizer=torch.optim.SGD(params=model.parameters(),weight_decay=weight_decay,lr=learning_rate)\n",
        "\n",
        "  #dataset_loading\n",
        "\n",
        "  training_data_load=DataLoader(dataset=dataset,shuffle=True,batch_size=32,pin_memory=True)\n",
        "  test_data_load=DataLoader(dataset=test,shuffle=False,batch_size=32,pin_memory=True)\n",
        "\n",
        "  #training loop\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    for batch_features,batch_labels in training_data_load:\n",
        "\n",
        "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "\n",
        "      #forward_propagation\n",
        "      prediction=model(batch_features)\n",
        "\n",
        "      #lloss_calculation\n",
        "      loss_output=loss(prediction,batch_labels)\n",
        "\n",
        "      #zero_optimizers\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #backward_prop\n",
        "      loss_output.backward()\n",
        "\n",
        "      #Optimize\n",
        "      optimizer.step()\n",
        "\n",
        "  #evaluation\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total=0\n",
        "  correct=0\n",
        "\n",
        "  #evaluation loop on test data\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for  batch_features,batch_labels in test_data_load:\n",
        "\n",
        "      batch_features,batch_labels=batch_features.to(device),batch_labels.to(device)\n",
        "\n",
        "      #forward_propagation\n",
        "      prediction=model(batch_features)\n",
        "\n",
        "      #finding the maximum probability\n",
        "      _,prediction=torch.max(prediction,1)\n",
        "\n",
        "      #total size\n",
        "      total+=batch_features.shape[0]\n",
        "\n",
        "      #correct predictions\n",
        "      correct+=(prediction==batch_labels).sum().item()\n",
        "\n",
        "  accuracy=correct/total\n",
        "\n",
        "  #return the accuracy to optimize\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLSjOr4fMLQf",
        "outputId": "814c78fe-06ef-4fba-f4b1-3436e2cbc008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-25 11:08:30,265] A new study created in memory with name: no-name-0d478771-2715-4844-a586-de5bd8cc20f7\n"
          ]
        }
      ],
      "source": [
        "import optuna as opt\n",
        "\n",
        "pruner=opt.pruners.MedianPruner(n_warmup_steps=5)\n",
        "\n",
        "study=opt.create_study(direction='maximize',pruner=pruner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "IaaeV3du6aZs",
        "outputId": "533157e7-f26d-4350-9840-6bc4c7f072ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "[I 2025-12-25 11:16:51,356] Trial 0 finished with value: 222.0 and parameters: {'neurons': 192, 'hidden_layers': 7, 'epochs': 80, 'lr': 0, 'weight_decay': 0.0002496458401999212}. Best is trial 0 with value: 222.0.\n",
            "[W 2025-12-25 11:21:10,509] Trial 1 failed with parameters: {'neurons': 80, 'hidden_layers': 4, 'epochs': 120, 'lr': 0, 'weight_decay': 0.0004430290196192507} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1283606674.py\", line 62, in objective_function\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 82, in _use_grad\n",
            "    ret = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/sgd.py\", line 127, in step\n",
            "    sgd(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/sgd.py\", line 304, in sgd\n",
            "    func(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/sgd.py\", line 352, in _single_tensor_sgd\n",
            "    grad = grad.add(param, alpha=weight_decay)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-12-25 11:21:10,514] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-433981754.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     ):\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1283606674.py\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0;31m#Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;31m#evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    125\u001b[0m             )\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             sgd(\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "study.optimize(objective_function,n_trials=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non Sequential Neural Networks\n"
      ],
      "metadata": {
        "id": "BLC_4nQcm8F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 1. Settings\n",
        "num_samples = 1000\n",
        "feat1_size = 10  # 10 Subjects (Academics)\n",
        "feat2_size = 5   # 5 Behavior Stats\n",
        "num_classes = 10 # Final Grade (0 to 9)\n",
        "\n",
        "# 2. Generate Structured Data (Patterns, not just noise)\n",
        "\n",
        "# --- Branch A: Academics (0.0 to 1.0 representing 0% to 100%) ---\n",
        "# We use a normal distribution so most students are \"average\" (0.5)\n",
        "data_x1_numpy = np.random.normal(loc=0.5, scale=0.15, size=(num_samples, feat1_size))\n",
        "# Clip values to ensure they stay between 0 and 1\n",
        "data_x1_numpy = np.clip(data_x1_numpy, 0, 1).astype(np.float32)\n",
        "\n",
        "# --- Branch B: Behavior (0.0 to 1.0) ---\n",
        "# Similar logic: most students have decent behavior\n",
        "data_x2_numpy = np.random.normal(loc=0.7, scale=0.1, size=(num_samples, feat2_size))\n",
        "data_x2_numpy = np.clip(data_x2_numpy, 0, 1).astype(np.float32)\n",
        "\n",
        "# 3. Create the Target (The \"Secret Formula\")\n",
        "# The Model has to LEARN this formula during training.\n",
        "# Logic: Grade is mostly based on Academics (70% weight) and Behavior (30% weight)\n",
        "avg_academics = np.mean(data_x1_numpy, axis=1) # Average score across 10 subjects\n",
        "avg_behavior = np.mean(data_x2_numpy, axis=1)  # Average behavior score\n",
        "\n",
        "# Calculate a \"Raw Score\" (0.0 to 1.0)\n",
        "raw_score = (0.7 * avg_academics) + (0.3 * avg_behavior)\n",
        "\n",
        "# Add a tiny bit of random noise (because real life isn't perfect)\n",
        "noise = np.random.normal(0, 0.02, size=num_samples)\n",
        "final_score = raw_score + noise\n",
        "\n",
        "# Convert 0.0-1.0 score to a Grade Class (0 to 9)\n",
        "# e.g., 0.95 -> Class 9, 0.1 -> Class 1\n",
        "targets_numpy = (final_score * 10).astype(np.int64)\n",
        "targets_numpy = np.clip(targets_numpy, 0, 9) # Ensure we don't go over 9\n",
        "\n",
        "# 4. Final Conversion to Tensors\n",
        "data_x1 = data_x1_numpy\n",
        "data_x2 = data_x2_numpy\n",
        "data_y = targets_numpy\n",
        "\n",
        "# --- Verification ---\n",
        "print(f\"Student 1 Academics Avg: {avg_academics[0]:.2f}\")\n",
        "print(f\"Student 1 Behavior Avg:  {avg_behavior[0]:.2f}\")\n",
        "print(f\"Student 1 Calculated Grade Class: {data_y[0]}\")\n",
        "print(\"\\nData is ready! There is a discoverable pattern (70% Academics + 30% Behavior).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_U4H-tkkdji",
        "outputId": "6926db24-40b2-4849-b0e3-5d0fbec44e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student 1 Academics Avg: 0.54\n",
            "Student 1 Behavior Avg:  0.63\n",
            "Student 1 Calculated Grade Class: 5\n",
            "\n",
            "Data is ready! There is a discoverable pattern (70% Academics + 30% Behavior).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_x1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2daW19jm03f",
        "outputId": "f434ecdc-57cf-4792-b282-e5304a2b95fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_x2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xOl-aC0m3vW",
        "outputId": "2246d85b-e434-44da-eac0-35f61d0d6c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else print('no'))"
      ],
      "metadata": {
        "id": "oQ7MqX52lvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUWvLBlol5Lw",
        "outputId": "d645de04-2ecb-419e-bd79-93bead210d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class CustomDatest(Dataset):\n",
        "  def __init__(self,features1,features2,labels):\n",
        "\n",
        "    self.features1=torch.tensor(features1,dtype=torch.float32,device=device)\n",
        "    self.features2=torch.tensor(features2,dtype=torch.float32,device=device)\n",
        "    self.labels=torch.tensor(labels,dtype=torch.long,device=device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features1)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    return self.features1[index],self.labels[index],self.features2[index]"
      ],
      "metadata": {
        "id": "qK1cpVZylErI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=CustomDatest(data_x1,data_x2,data_y)"
      ],
      "metadata": {
        "id": "_0gnOnXcldWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_load_test=DataLoader(training_data,batch_size=10,shuffle=True)"
      ],
      "metadata": {
        "id": "pMacf7zGolQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jAasVeR7BDR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, number_of_features1, number_of_features2):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # --- BRANCH 1 (Academics) ---\n",
        "        # FIX: Used nn.Sequential instead of a list []\n",
        "        self.model1 = nn.Sequential(\n",
        "            nn.Linear(number_of_features1, 182),\n",
        "            nn.BatchNorm1d(182),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            nn.Linear(182, 20),\n",
        "            nn.BatchNorm1d(20),  # FIX: Changed 182 to 20 to match previous layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),   # FIX: Removed extra 'nn.'\n",
        "\n",
        "            nn.Linear(20, 4)     # Output is 4 features\n",
        "        )\n",
        "\n",
        "        # --- BRANCH 2 (Behavior) ---\n",
        "        # FIX: Input should likely be 'number_of_features2', not 1\n",
        "        self.model2 = nn.Sequential(\n",
        "            nn.Linear(number_of_features2, 182),\n",
        "            nn.BatchNorm1d(182),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(182, 20)   # Output is 20 features\n",
        "        )\n",
        "\n",
        "        # --- FINAL MERGED LAYER ---\n",
        "        # Input = Output of Model 1 (4) + Output of Model 2 (20) = 24\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Linear(24, 200),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(200, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_features, y_features):\n",
        "        # 1. Process Branch A\n",
        "        output1 = self.model1(x_features)  # Shape: [Batch, 4]\n",
        "\n",
        "        # 2. Process Branch B\n",
        "        output2 = self.model2(y_features)  # Shape: [Batch, 20]\n",
        "\n",
        "        # 3. THE MERGE (Concatenate, don't Add)\n",
        "        # Glue them together side-by-side (dim=1)\n",
        "        # Result Shape: [Batch, 24]\n",
        "        combined_output = torch.cat((output1, output2), dim=1)\n",
        "\n",
        "        # 4. Final Prediction\n",
        "        final_output = self.last_layer(combined_output)\n",
        "\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn(epochs,lr,data_load_test=data_load_test):\n",
        "\n",
        "    model = NeuralNetwork(number_of_features1=10, number_of_features2=5)\n",
        "\n",
        "    model.to(device=device)\n",
        "\n",
        "    Loss=nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
        "\n",
        "    for epocs in range(epochs):\n",
        "\n",
        "      total_loss=0\n",
        "\n",
        "      for batch_x1,batch_x2,batch_labels in data_load_test:\n",
        "\n",
        "                #forward_pass\n",
        "                output=model(batch_x1,batch_x2)\n",
        "\n",
        "                #loss\n",
        "                loss=Loss(output,batch_labels)\n",
        "\n",
        "                #backprop and zero_grad\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                #optimize\n",
        "                optimizer.step()\n",
        "\n",
        "\n",
        "                total_loss+=loss.item()\n",
        "\n",
        "\n",
        "      # Calculate the average loss for the current epoch.\n",
        "      avg_loss=total_loss/len(data_load_test)\n",
        "\n",
        "      # Print the epoch number and the average training loss.\n",
        "      print(f'epoch {epocs+1} and the avg loss {avg_loss:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "dzojCH9fn4KR",
        "outputId": "1f977b21-2f2e-4e90-82d9-e2623c0d8aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_load_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3367705416.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_load_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_load_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_features1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_features2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_load_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trained_model = train_nn(epochs=50, lr=0.01)"
      ],
      "metadata": {
        "id": "jeEHJKefrCqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((torch.tensor([1,2,3]),torch.tensor([12,3,4])),dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNXO4fmTrqyj",
        "outputId": "6ca80be7-53f3-4c7f-838b-00fce41ce5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3, 12,  3,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create two identical tensors\n",
        "# Shape: [2 rows, 3 columns]\n",
        "t1 = torch.tensor([[1, 2, 3],\n",
        "                   [4, 5, 6]])\n",
        "\n",
        "t2 = torch.tensor([[10, 20, 30],\n",
        "                   [40, 50, 60]])"
      ],
      "metadata": {
        "id": "S3Leugdrts8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((t1,t2),dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z19qBiTpuNN_",
        "outputId": "9456184e-931d-47c3-82e5-57729d203093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3, 10, 20, 30],\n",
              "        [ 4,  5,  6, 40, 50, 60]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((t1,t2),dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXt4HIOYuPlu",
        "outputId": "0d935cf1-9857-471c-c5b6-170a41152f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3],\n",
              "        [ 4,  5,  6],\n",
              "        [10, 20, 30],\n",
              "        [40, 50, 60]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1.reshape(1,3,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji1N6MCUuZ71",
        "outputId": "155cbd06-cc68-47c2-aaff-5cf05704c06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 2],\n",
              "         [3, 4],\n",
              "         [5, 6]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnv5lxZmqCYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNlnzcQCYDoiaEtJztP5Y3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}